<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hybrid-Granularity Image-Music Retrieval</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1, h2, h3 {
            text-align: center;
            margin: 20px 0;
        }
        h1 {
            font-size: 2em;
        }
        h2 {
            font-size: 1.5em;
        }
        h3 {
            font-size: 1.2em;
        }
        .authors {
            text-align: center;
            margin-bottom: 20px;
        }
        .abstract {
            background-color: #f9f9f9;
            border-left: 5px solid #ccc;
            padding: 10px 15px;
            margin-bottom: 30px;
        }
        .section {
            margin-bottom: 30px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 30px;
        }
        table, th, td {
            border: 1px solid #ddd;
        }
        th, td {
            padding: 10px;
            text-align: center;
        }
        figcaption {
            text-align: center;
            font-style: italic;
            margin-top: 5px;
        }
        figure {
            text-align: center;
            margin-bottom: 30px;
        }
        figure#large-space {
            margin-top: 50px;
            margin-bottom: 50px; /* Increased space around the figure */
        }
        footer {
            text-align: center;
            margin-top: 50px;
        }
        .github-link {
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 20px;
        }
        .github-icon {
            width: 24px; /* Adjust size as needed */
            height: 24px;
            margin-right: 5px;
        }
    </style>
</head>
<body>

<h1>Hybrid-Granularity Image-Music Retrieval Using Contrastive Learning between Images and Music</h1>

<div class="github-link">
    <a href="https://github.com/Blossomers/HG-CLIM" target="_blank">
        <img src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub" class="github-icon">
    </a>
</div>

<div class="authors">
    <p>Xudong He<sup>1</sup>, Li Wang<sup>1</sup>, Zhao Wang<sup>1</sup>, Jun Xiao<sup>1</sup></p>
    <p><sup>1</sup>Zhejiang University</p>
</div>

<h2>Abstract</h2>
<div class="abstract">
    <p>Cross-modal music retrieval remains a challenging task for current search engines. Existing engines match music tracks using coarse-granularity retrieval of metadata, like pre-defined tags and genres. These methods face difficulties handling fine-granularity contextual queries. We propose a novel dataset of 66,048 image-music pairs for cross-modal music retrieval and introduce a hybrid-granularity retrieval framework using contrastive learning. Our method outperforms previous approaches, ensuring superior image-music alignment.</p>
</div>

<h2>1. Introduction</h2>
<div class="section">
    <p>Large-scale music platforms often use metadata-based search engines, but these methods struggle with context-specific queries. To solve this issue, we present a novel approach that learns hybrid-granularity context alignment between images and music through contrastive learning.</p>
</div>

<h2>2. The MIPNet Dataset</h2>
<div class="section">
    <p>We created the MIPNet dataset, consisting of 66,048 image-music pairs. Each pair includes an image and a corresponding 10-second music clip, with both being labeled for emotional context. These pairs enable the training of cross-modal retrieval models to better align images with their associated music tracks.</p>
</div>

<figure>
    <img src="dataset.png" alt="Image-Music Retrieval Example" width="600">
    <figcaption>Figure 1: Examples of prior image-based music retrieval methods mismatching unrelated contexts.</figcaption>
</figure>

<h2>3. The HG-CLIM Framework</h2>
<div class="section">
    <p>The HG-CLIM framework incorporates both coarse-granularity and fine-granularity retrieval methods. It utilizes ConvNext and PaSST encoders to extract image and music features, which are projected into a shared embedding space for better cross-modal alignment.</p>
</div>

<figure id="large-space">
    <img src="Framework.png" alt="HG-CLIM Framework" width="600">
    <figcaption>Figure 2: Overview of the HG-CLIM framework.</figcaption>
</figure>

<h2>4. Experimental Results</h2>
<div class="section">
    <h3>4.1 Results on MIPNet Dataset</h3>
    <p>We tested our HG-CLIM framework on the MIPNet dataset using various metrics such as MRR, R@10, and P@1. The results show that our method significantly outperforms previous models like EMO-CLIM and VM-NET.</p>
    <table>
        <thead>
            <tr>
                <th>Method</th>
                <th>MRR</th>
                <th>R@10</th>
                <th>P@1</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>EMO-CLIM</td>
                <td>0.0804 / 0.0812</td>
                <td>0.1592 / 0.1633</td>
                <td>0.0831 / 0.0791</td>
            </tr>
            <tr>
                <td>VM-NET</td>
                <td>0.3279 / 0.3165</td>
                <td>0.6463 / 0.6258</td>
                <td>0.2001 / 0.2057</td>
            </tr>
            <tr>
                <td>HG-CLIM (ours)</td>
                <td>0.5124 / 0.5104</td>
                <td>0.8080 / 0.8082</td>
                <td>0.2931 / 0.2910</td>
            </tr>
        </tbody>
    </table>
    
    <h3>4.2 Results on Emotion-aligned Music Retrieval</h3>
    <p>Our method also shows compeitive abilitiy in emotion-based music retrieval task.</p>
    <table>
        <thead>
            <tr>
                <th>Method</th>
                <th>MRR</th>
                <th>R@10</th>
                <th>P@1</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>MMTS</td>
                <td>0.4575 / 0.4807</td>
                <td>0.6887 / 0.7123</td>
                <td>0.4070 / 0.4188</td>
            </tr>
            <tr>
                <td>EMO-CLIM</td>
                <td>0.4619 / 0.5072</td>
                <td>0.8237 / 0.7986</td>
                <td>0.4917 / 0.4935</td>
            </tr>
            <tr>
                <td>HG-CLIM (ours)</td>
                <td>0.4765 / 0.5123</td>
                <td>0.8215 / 0.7921</td>
                <td>0.5033 / 0.5094</td>
            </tr>
        </tbody>
    </table>

    <h3>4.3 Ablation Studies</h3>
    <p>The results of the ablation studies demonstrate that the fine-granularity loss significantly aids the model in learning implicit context-specific information.</p>
    <table>
        <thead>
            <tr>
                <th>Loss</th>
                <th>MRR (I → M)</th>
                <th>MRR (M → I)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Baseline</td>
                <td>0.3164</td>
                <td>0.2662</td>
            </tr>
            <tr>
                <td>Baseline + Lintra<sub>fine</sub></td>
                <td>0.3272</td>
                <td>0.3196</td>
            </tr>
            <tr>
                <td>Baseline + Linter<sub>fine</sub></td>
                <td>0.4793</td>
                <td>0.4761</td>
            </tr>
            <tr>
                <td>Baseline + L<sub>fine</sub> (HG-CLIM)</td>
                <td>0.5124</td>
                <td>0.5104</td>
            </tr>
        </tbody>
    </table>
</div>

<h2>5. Conclusion</h2>
<div class="section">
    <p>Our work introduces HG-CLIM, a hybrid-granularity context alignment framework for image-based music retrieval. This approach enables retrieval tasks that align images and music on both coarse and fine-granularity levels, demonstrating state-of-the-art performance.</p>
</div>

<footer>
    <p>&copy; 2024 Zhejiang University. All rights reserved.</p>
</footer>

</body>
</html>
